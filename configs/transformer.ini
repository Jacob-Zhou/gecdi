[model]
encoder = 'transformer'
feat = []
n_embed = 512
pos = 'sinusoid'
token_dropout = .1
embed_dropout = .1
n_encoder_layers = 6
n_encoder_heads = 8
n_encoder_hidden = 512
n_encoder_inner = 2048
encoder_attn_dropout = .1
encoder_ffn_dropout = .1
encoder_dropout = .1
n_decoder_layers = 6
n_decoder_heads = 8
n_decoder_hidden = 512
n_decoder_inner = 2048
decoder_attn_dropout = .1
decoder_ffn_dropout = .1
decoder_dropout = .1
beam_size = 12
max_len = 512
length_penalty = 1.
topk = 1
find_unused_parameters = 0

[optim]
lr = 5e-04
mu = .9
nu = .98
eps = 1e-8
weight_decay = 0
clip = .1
min_freq = 2
fix_len = 20
warmup_steps = 4000
update_steps = 5
epochs = 60
patience = 10
batch_size = 65536
label_smoothing = 0.1